###### -*- coding: ASCII -*-
#REF:https://github.com/apache/spark/blob/master/examples/src/main/python/mllib/binary_classification_metrics_example.py
from __future__ import print_function
from pyspark import SparkConf, SparkContext
from time import time, strftime, localtime
import os, sys
from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel
from pyspark.mllib.evaluation import BinaryClassificationMetrics
from pyspark.mllib.feature import Normalizer as NL
from pyspark.mllib.feature import PCA, StandardScaler, StandardScalerModel
from pyspark.mllib.tree import DecisionTree, DecisionTreeModel, RandomForest, RandomForestModel, GradientBoostedTrees, GradientBoostedTreesModel
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.linalg import Vectors
from pyspark.mllib.util import MLUtils

# Globol Parameters Here
if True:
	# if you take the 100m-splited file, remember to modify here.
	DataSplit='15m_balanced_95/5'
	# *****Main Parameters*****
	PredictLABEL = 'booking'	#	'booking' OR 'click'
	algorithmMETHOD = 'DT'	# RF, GBT, NB or DT
	# *****Tree Parameters*****
	NumTrees = 10 #Bigger you set, longer your program run.
	NumMaxTreeDepth = 15 #max is 30, if it set to high, it could cause memory run out.
	NumMaxTreeBins = 32
	LearningRate = 0.5
	# *****Feature Engineering Setting*****
	feature_Engineering = 'none' # Normalizer, StandardScaler,  PCA or none.
	normalize_P = '2'	#as Default
	NumPCA = 30 # if it doesn't work, decrease its value.
	# *****File imported*****
	# FILE = '/home/user/Desktop/train_100m.csv'
	FILE = 'G:\\dataset\\kaggle_expedia-personalized-sort\\train_15m.csv'	#for my PC @ home
	# FILE = 'D:\\dataset\\expedia\\train_15m.csv'	#for my poor Laptop

def CreateSparkContext():
	sparkConf = SparkConf()                                            \
						 .setAppName("NCCUBigDataProjFinal-pipe")           \
						 .set("spark.ui.showConsoleProgress", "false") 
	sc = SparkContext(conf = sparkConf)
	print ("master="+sc.master)    
	SetLogger(sc)
#    SetPath(sc)
	return (sc)

def SetLogger(sc):
	logger = sc._jvm.org.apache.log4j
	logger.LogManager.getLogger("org"). setLevel( logger.Level.ERROR )
	logger.LogManager.getLogger("akka").setLevel( logger.Level.ERROR )
	logger.LogManager.getRootLogger().setLevel(logger.Level.ERROR)

'''
	def SetPath(sc):
		global Path
		if sc.master[0:5]=="local" :
			Path="file:/home/cs/Demo/"
		else:   
			Path="hdfs://master:9000/data/"

# Logistic Regression only
def SaveOrLoadModel(model = "" , folder = ("model_" + str(time())) , option = 's'):
	if (option == 's'):
		model.save(sc, folder)
	elif (option == 'l'):
		return LogisticRegressionModel.load(sc, folder)
'''
def gettime(t = time(), method = ""):
	if(method == "f"):	#f means floder
		return strftime("%Y-%m-%d-%H-%M-%S(+0800)", localtime(t))
	else:
		return strftime("%Y-%m-%d %H:%M:%S(+0800)", localtime(t))

def normalizer(LPRDD, P=normalize_P):
	lpF = LPRDD.map(lambda x : x.features)
	lpL = LPRDD.map(lambda x : x.label)
	nl = NL(p=P) # 
	return lpL.zip(nl.transform(lpF)).map(lambda x: LabeledPoint(x[0],x[1]))

def PCAer(LPRDD, P=NumPCA):
	lpF = LPRDD.map(lambda x : x.features)
	lpL = LPRDD.map(lambda x : x.label)
	model = PCA(P).fit(lpF)
	PCAed = model.transform(lpF)
	return lpL.zip(PCAed).map(lambda x: LabeledPoint(x[0],x[1]))

def StandardScaleR(LPRDD):
	lpF = LPRDD.map(lambda x : x.features)
	lpL = LPRDD.map(lambda x : x.label)
	model =StandardScaler().fit(lpF)
	scalered = model.transform(lpF)
	return lpL.zip(scalered).map(lambda x: LabeledPoint(x[0],x[1]))

def FeaTureEngineerer(lpRDD):
	if (feature_Engineering == 'StandardScaler'):
		lpRDD = StandardScaleR(trainlpRDDs)
	elif (feature_Engineering == 'Normalizer'):
		lpRDD = normalizer(trainlpRDDs)
	elif (feature_Engineering == 'PCA'):
		lpRDD = PCAer(trainlpRDDs)
	return lpRDD

def MeanByKey(lpRDD, index_K, index_V):
	return lpRDD.map(lambda x:(x[index_K] , x[index_V])).reduceByKey(lambda x,y: (float(x) + float(y))/2  )

def DictMedianByKey(lpRDD, index_K, index_V):
	Dict = {}
	# x, y should be trans 2 strings
	# the Keys will be string @ final
	# the Value will be float @ final
	lpRDD = lpRDD.map(lambda x:(x[index_K] , x[index_V]))
	lpRDD = lpRDD.reduceByKey(lambda x,y :(str(x) + ',' + str(y)))
	lpRDD = lpRDD.map(lambda x:(x[0], sorted(list(float(a) for a in x[1].split(',')))))
	lpRDD = lpRDD.map(lambda x :(x[0], x[1][len(x[1])/2]))
	for (k, v) in lpRDD.collect():
		Dict[k] = v
	return Dict
	
def LoadAndPrepare(dataset=FILE, LABEL = PredictLABEL):
	# replace"NULL" by -2 to make it a unique feature
	# delete the header
	# split by ','
	f = sc.textFile(dataset).map(lambda x: str(x).replace('NULL','-2')) #GIVE -2 to distinguish null
	header = f.first()
	f = f.filter(lambda x: x != header).map(lambda x : x.split(","))
	print (f.first())
	
	# The mdeian price of the LOCATE country
	DictLocalCtyPriceGap = DictMedianByKey(f, 3, 5)
	# The mdeian price of the DESTINATION country
	DictDestCtyPriceGap = DictMedianByKey(f, 6, 15)
	# The mdeian price of the DESTINATION Site
	DictDestPriceGap = DictMedianByKey(f, 17, 15)
	# The mdeian price of the hotel ever
	DictEverPriceGap = DictMedianByKey(f, 7, 15)
	
	# feature dealing & extension
	# Dealing with x[1] = Date&Time
	# Count the whole competitors
	f = f.map(lambda x:\
		      ([ int(x[1].split()[0].split('-')[0]) - 2013 + ( float(x[1].split()[0].split('-')[1]) * 30 + int(x[1].split()[0].split('-')[2])) / 365 ] + \
			   [int(x[1].split()[1].split(':')[0]) * 60 + int(x[1].split()[1].split(':')[1]) ] + \
			   x[2:-3] + \
			   # feature[51], the competitor NULLs
			   [ sum(1 for a in x[27:51] if int(a) == -2) ] + \
			   # feature[52] , the gap (by percent) between the price searched and the searcher's expect
			   [ (float(x[15]) - float(x[5])) / (float(x[5])  if float(x[5]) !=0 else 100)   ] + \
			   # feature[53] , the gap (by percent) between the price searched and the median of Local Country
			   [  ( float(x[15]) - DictEverPriceGap[x[7]] / (float(x[15])  if float(x[15]) !=0 else 100)  )] + \
			   # feature[54] , the gap (by percent) between the price searched and the median of Local Country
			   [  ( float(x[15]) - DictLocalCtyPriceGap[x[3]] / (float(x[15])  if float(x[15]) !=0 else 100)   )] + \
			   # feature[55] , the gap (by percent) between the price searched and the median of destination Country
			   [  ( float(x[15]) - DictDestCtyPriceGap[x[6]] / (float(x[15])  if float(x[15]) !=0 else 100)   )] + \
			   # feature[56] , the gap (by percent) between the price searched and the median of destination site
			   [  ( float(x[15]) - DictDestPriceGap[x[17]] / (float(x[15])  if float(x[15]) !=0 else 100)   )] + \
			   # feature[57] , the gap (by percent) between the the median of Local Country and the median of destination Country
			   [  ( (DictLocalCtyPriceGap[x[3]] - DictDestCtyPriceGap[x[6]]) / DictLocalCtyPriceGap[x[3]]  )] + \
			   # feature[58] , the gap (by percent) between the the median of Local Country and the median of destination Country
			   [  ( DictDestPriceGap[x[17]] - DictDestCtyPriceGap[x[6]] / DictDestCtyPriceGap[x[6]]  )] + \
			   # feature[59] , the proportion of children
			   # [   float(x[21]) / (float(x[20]) + float(x[21]))  if (float(x[20]) + float(x[21]) > 0) else -2 ] + \
			   # Labels [60 & 61]
			   [x[-3] , x[-1]]
			 ))
	f = f.map(lambda x:\
		      (x[:3] + [  DictLocalCtyPriceGap[x[3]] ] + \
			   x[4:6] + [ DictDestCtyPriceGap[x[6]] , DictEverPriceGap[x[7]] ] + \
			   x[8:17] + [DictDestPriceGap[x[17]]] + \
			   x[18:] \
			 ))
	#no need to trans 2 float cause mllib.regression will do that
	print (f.first())
	if LABEL == 'booking':
		f = f.map(lambda x: LabeledPoint(x[-1] , x[:-2]))	#x[-1] is booking
	else:
		f = f.map(lambda x: LabeledPoint(x[-2] , x[:-2]))	#x[-2] is click
	print (f.first().label)
	print (f.first().features)
	return f

def ML(lpRDD, algo = algorithmMETHOD):
	
	if (algorithmMETHOD == 'RF'):
		model = RandomForest.trainClassifier(trainlpRDDs, numClasses=2, categoricalFeaturesInfo={},\
						numTrees=NumTrees, featureSubsetStrategy="auto",\
						impurity='gini', maxDepth=NumMaxTreeDepth, maxBins=NumMaxTreeBins)
	elif (algorithmMETHOD == 'NB'):
		model = NaiveBayes.train(trainlpRDDs , 1.0)
	elif (algorithmMETHOD == 'GBT'):
		model = GradientBoostedTrees.trainClassifier(trainlpRDDs, categoricalFeaturesInfo={},\
						maxDepth=NumMaxTreeDepth, maxBins=NumMaxTreeBins, learningRate=LearningRate, numIterations=NumTrees)
	elif (algorithmMETHOD == 'DT'):
		model = DecisionTree.trainClassifier(trainlpRDDs, numClasses=2, categoricalFeaturesInfo={},
									 impurity='gini', maxDepth=NumMaxTreeDepth, maxBins=NumMaxTreeBins)
	return model

if __name__ == "__main__":
	sc = CreateSparkContext()
	StartTime = time()
	SavePath = gettime(StartTime, "f")

	print("============= Loading ================" + gettime(time()))
	f = LoadAndPrepare(FILE, PredictLABEL)
	# Make 1s & 0s balanced.
	
	(droped, testlpRDD) = f.randomSplit([95, 5])
	
	d1s = droped.filter(lambda x: x.label == '1.0')
	d0s = droped.filter(lambda x: x.label == '0.0')
	(d0s_splited, droped) = droped.randomSplit([3, 97])
	trainlpRDDs = d1s.union(d0s_splited)
	
	# 3 feature engineer
	trainlpRDDs = FeaTureEngineerer(trainlpRDDs)
	
	print("============= Training ===============" + gettime(time()))
	model = ML(trainlpRDDs , algorithmMETHOD)
	ModelingDuration = time() - StartTime
	
	print("============= Predicting =============" + gettime(time()))
	prediction = model.predict(testlpRDD.map(lambda x: x.features))

	print("============= Computing ==============" + gettime(time()))
	PredictionsAndLabel = prediction.zip(testlpRDD.map(lambda lp: lp.label))
	ErrCount = PredictionsAndLabel.filter(lambda lp: (lp[0] != lp[1])).count()
	ModelMetric = BinaryClassificationMetrics(PredictionsAndLabel)
	trainCount= trainlpRDDs.count()
	TestCount = testlpRDD.count()
	ErrRate = float(ErrCount) / TestCount * 100
	
	print("============= Saving =================" + gettime(time()))
	with open('info.txt' , 'a') as finfo:
		print(str(gettime(StartTime)) , file = finfo)
		print("Label                 :" + PredictLABEL , file = finfo)
		print('Method:               :' + algorithmMETHOD , file = finfo)
		print("DataSplit             :" + DataSplit , file = finfo)
		print("FeatureEngineer       :" + feature_Engineering , file = finfo)
		if algorithmMETHOD == 'GBT' or algorithmMETHOD == 'RF':
			print("NumTrees              :" + str(NumTrees), file = finfo)
		if algorithmMETHOD != 'NB':
			print("MaxTreeDepth          :" + str(NumMaxTreeDepth) , file = finfo)
			print("MaxTreeBins           :" + str(NumMaxTreeBins) , file = finfo)
		if algorithmMETHOD == 'GBT':
			print("LearningRate          :" + str(LearningRate) , file = finfo)
		print("train                 :" + str(trainCount) , file = finfo)
		print("AUC                   :" + str(ModelMetric.areaUnderROC) , file = finfo)
		print("APR                   :" + str(ModelMetric.areaUnderPR) , file = finfo)
		print("ErrorCount            :" + str(ErrCount) , file = finfo)
		print("ErrorRate             :" + str(ErrRate) + "%" , file = finfo)
		print("ModelingDuration      :" + str(ModelingDuration) , file = finfo)
		print("TotalDuration :"+str(time()-StartTime) + "\n" , file = finfo)
		print("Tree                  :" + model.toDebugString() , file = finfo)
	'''
	for i in range(len(PredictionsAndLabels)):
		with open(("PredictionsAndLabels" + str(i + 1) + ".csv") , 'a') as fPrediction:
			for (prediction, label) in PredictionsAndLabels[i].collect():
				print(str(prediction) + "," + str(label), file=fPrediction)
	'''
	print("============= Printing ===============" + gettime(time()))
	print("Label                 :" + PredictLABEL)
	print('Method:               :' + algorithmMETHOD)
	print("DataSplit             :" + DataSplit)
	print("FeatureEngineer       :" + feature_Engineering)
	if algorithmMETHOD != 'NB':
		print("NumTrees              :" + str(NumTrees))
		print("MaxTreeDepth          :" + str(NumMaxTreeDepth))
		print("MaxTreeBins           :" + str(NumMaxTreeBins))
	print("train                 :" + str(trainCount))
	print("AUC                   :" + str(ModelMetric.areaUnderROC))
	print("APR                   :" + str(ModelMetric.areaUnderPR))
	print("ErrorCount            :" + str(ErrCount))
	print("ErrorRate             :" + str(ErrRate) + "%")
	print("ModelingDuration      :" + str(ModelingDuration))

	print("============= Done ===================" + gettime(time()))
	print("TotalDuration :"+str(time()-StartTime))
	sc.stop()
	
